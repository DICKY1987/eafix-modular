---
doc_id: DOC-PAT-BATCH-DOC-ID-ASSIGNMENT-001
pattern_id: PAT-BATCH-DOC-ID-001
version: 1.0.1
category: automation
status: active
created: 2026-01-09
---

# Pattern: Batch Doc ID Assignment (eafix-modular)
# Pattern ID: PAT-BATCH-DOC-ID-001
# Version: 1.0.1
# Category: automation
# Status: active

name: "batch_doc_id_assignment"
intent: |
  Rapidly assign doc_ids to many eligible files using batch processing
  with registry updates and validation checkpoints.

applicability:
  when_to_use:
    - Need to assign doc_ids to 100+ files
    - Files already scanned into docs_inventory.jsonl
    - Registry infrastructure is operational
    - Coverage target needs to be reached quickly

  when_not_to_use:
    - Single file doc_id assignment (use doc_id_assigner.py single)
    - Registry is corrupt or needs cleanup first
    - Files are not in inventory yet

inputs:
  inventory_path:
    type: string
    required: true
    default: "doc_id_subsystem/registry/docs_inventory.jsonl"
    description: "JSONL inventory of all eligible files"

  registry_path:
    type: string
    required: true
    default: "doc_id_subsystem/registry/DOC_ID_REGISTRY.yaml"
    description: "Central doc_id registry (auto-updated by assigner)"

  batch_size:
    type: integer
    required: false
    default: 500
    minimum: 50
    maximum: 1000
    description: "Number of files to process per batch"

  file_type_filter:
    type: array
    required: false
    items:
      type: string
      enum: ["py", "yaml", "yml", "json", "md", "ps1", "sh", "txt", "toml", "bat"]
    description: "Restrict to specific file types (empty = all)"

  target_coverage:
    type: float
    required: false
    default: 1.0
    minimum: 0.0
    maximum: 1.0
    description: "Stop when this coverage percentage is reached"

  dry_run:
    type: boolean
    required: false
    default: false
    description: "Preview assignments without writing files"

outputs:
  assignment_report:
    type: object
    properties:
      total_missing_before: integer
      processed: integer
      assigned: integer
      skipped: integer
      coverage_before: float
      coverage_after: float
      duration_seconds: float
      batches_completed: integer

  registry_updated:
    type: boolean
    description: "Whether registry was updated (false for dry-run)"

  validation_status:
    type: object
    properties:
      invalid_doc_ids: integer
      duplicate_doc_ids: integer
      coverage_passes_baseline: boolean

execution_steps:
  - step_id: "S1_pre_validation"
    description: "Validate system state before batch processing"
    parallel: false

    actions:
      - action: "scan_repository"
        command: "python doc_id_subsystem/core/doc_id_scanner.py"
        description: "Refresh inventory with current file state"
        expected_output: "Inventory saved"

      - action: "check_registry_health"
        checks:
          - file_exists: "doc_id_subsystem/registry/DOC_ID_REGISTRY.yaml"
          - file_size_gt: 100000
          - yaml_valid: true

      - action: "baseline_coverage"
        command: "python doc_id_subsystem/core/doc_id_scanner.py --stats"
        capture: ["coverage_before", "total_eligible", "files_with_id"]

    ground_truth_verification:
      - type: "system_ready"
        check: "inventory exists and registry is valid"

    reporting:
      on_success: "OK pre-validation passed: {coverage_before}% coverage"
      on_failure: "Pre-validation failed: {error}"

  - step_id: "S2_batch_assignment_loop"
    description: "Assign doc_ids in batches until target reached"
    parallel: false
    loop: true
    loop_condition: "coverage < target_coverage AND files_remaining > 0"
    max_iterations: 20

    actions:
      - action: "assign_batch"
        command: "python doc_id_subsystem/core/doc_id_assigner.py auto-assign --limit {batch_size}"
        parameters:
          - limit: "{batch_size}"
        timeout_seconds: 300

      - action: "refresh_scan"
        command: "python doc_id_subsystem/core/doc_id_scanner.py"
        description: "Update inventory with new assignments"

      - action: "check_duplicates"
        command: "python doc_id_subsystem/validation/fix_duplicate_doc_ids.py analyze"
        expect_output: "Found 0 unique doc_ids with duplicates"
        on_failure: "abort_with_report"

      - action: "check_invalids"
        command: "python doc_id_subsystem/validation/fix_invalid_doc_ids.py --dry-run"
        expect_output: "No invalid doc_ids found"
        on_failure: "abort_with_report"

      - action: "update_coverage_metric"
        command: "python doc_id_subsystem/core/doc_id_scanner.py --stats"
        capture: ["coverage_current", "files_with_id_current"]

    ground_truth_verification:
      - type: "batch_success"
        check: "files_assigned > 0 for this batch"

      - type: "quality_gate"
        check: "invalid_count == 0 AND duplicate_count == 0"

    reporting:
      on_iteration: "Batch {iteration}: Assigned {files_assigned}, Coverage: {coverage_current}%"
      on_success: "Batch {iteration} complete"
      on_failure: "Batch {iteration} failed: {error}"

  - step_id: "S3_type_specific_targeting"
    description: "Target specific file types to fill coverage gaps"
    parallel: false
    conditional: "coverage < target_coverage"

    actions:
      - action: "identify_gap_types"
        logic: |
          Find file types with lowest coverage in stats output.
        output: "priority_types"

      - action: "assign_type_batch"
        command: "python doc_id_subsystem/core/doc_id_assigner.py auto-assign --limit 200 --types {priority_types}"
        description: "Target batches by file type"

    ground_truth_verification:
      - type: "gap_reduction"
        check: "coverage increased for targeted types"

    reporting:
      on_success: "Type-specific targeting complete"

  - step_id: "S4_final_validation"
    description: "Comprehensive validation and compliance check"
    parallel: false

    actions:
      - action: "final_scan"
        command: "python doc_id_subsystem/core/doc_id_scanner.py"

      - action: "validate_coverage"
        command: "python doc_id_subsystem/validation/validate_doc_id_coverage.py --baseline {target_coverage}"
        expect_exit_code: 0

      - action: "duplicate_check"
        command: "python doc_id_subsystem/validation/fix_duplicate_doc_ids.py analyze"
        expect_output: "Found 0 unique doc_ids with duplicates"

      - action: "generate_final_stats"
        command: "python doc_id_subsystem/core/doc_id_scanner.py --stats"
        capture: ["coverage_final", "total_docs", "by_type_coverage"]

    ground_truth_verification:
      - type: "coverage_achieved"
        check: "coverage_final >= target_coverage"

      - type: "quality_maintained"
        check: "invalid_count == 0 AND duplicate_count == 0"

      - type: "registry_integrity"
        check: "registry file valid and total_docs matches inventory count"

    reporting:
      on_success: |
        BATCH DOC_ID ASSIGNMENT COMPLETE
        - Coverage: {coverage_before}% -> {coverage_final}%
        - Files assigned: {files_assigned_total}
        - Batches processed: {batches_completed}
        - Duration: {duration_seconds}s
        - Quality: 0 invalid, 0 duplicates
      on_failure: "Final validation failed: {error}"

ground_truth_criteria:
  required:
    - criterion: "target_coverage_achieved"
      description: "Coverage meets or exceeds target baseline"
      verification: "validate_doc_id_coverage.py"

    - criterion: "zero_quality_issues"
      description: "No invalid or duplicate doc_ids"
      verification: "scanner stats + duplicate analyzer"

    - criterion: "registry_synchronized"
      description: "Registry contains entries for all assigned doc_ids"
      verification: "registry count matches inventory count"

error_handling:
  - error_type: "duplicate_detected"
    response: "abort_batch"
    action: "run fix_duplicate_doc_ids.py fix"
    retry_after_fix: true

  - error_type: "invalid_detected"
    response: "abort_batch"
    action: "run fix_invalid_doc_ids.py"
    retry_after_fix: true

  - error_type: "registry_corruption"
    response: "abort"
    action: "restore from backup or rebuild"
    manual_intervention: true

performance_profile:
  estimated_time_per_500_files: 180
  estimated_time_2000_files: 720
  estimated_time_5000_files: 1800

  files_per_second: 2.5
  registry_updates_per_file: 1

  bottlenecks:
    - file_io_writes
    - yaml_serialization
    - content_parsing

optimization_strategies:
  - strategy: "parallel_batches"
    description: "Process multiple batches in parallel (requires thread-safe registry)"
    potential_speedup: "3x"
    implementation_complexity: "high"

  - strategy: "bulk_registry_update"
    description: "Batch registry writes instead of per-file"
    potential_speedup: "2x"
    implementation_complexity: "medium"

  - strategy: "skip_scan_between_batches"
    description: "Only scan at start and end"
    potential_speedup: "1.5x"
    implementation_complexity: "low"
    risk: "delayed error detection"

tool_bindings:
  powershell:
    invoke: |
      # Batch assignment to target coverage (run from repo root)
      $target = 1.0
      $maxBatches = 10

      for ($i = 1; $i -le $maxBatches; $i++) {
          Write-Host "==> Batch $i of $maxBatches"

          # Assign batch
          python doc_id_subsystem/core/doc_id_assigner.py auto-assign --limit 500

          # Refresh inventory and check coverage
          python doc_id_subsystem/core/doc_id_scanner.py
          $stats = python doc_id_subsystem/core/doc_id_scanner.py --stats 2>&1
          if ($stats -match "Coverage: (\\d+\\.\\d+)%") {
              $coverage = [float]$matches[1] / 100
              Write-Host "Coverage: $($coverage * 100)%"

              if ($coverage -ge $target) {
                  Write-Host "Target coverage reached"
                  break
              }
          }

          # Validate quality
          $dupes = python doc_id_subsystem/validation/fix_duplicate_doc_ids.py analyze 2>&1
          if ($dupes -notmatch "Found 0 unique doc_ids with duplicates") {
              Write-Error "Duplicates detected - aborting"
              break
          }
      }

      # Final report
      python doc_id_subsystem/core/doc_id_scanner.py --stats

  python:
    invoke: |
      import subprocess
      import time

      TARGET_COVERAGE = 1.0
      BATCH_SIZE = 500
      MAX_BATCHES = 10

      for i in range(1, MAX_BATCHES + 1):
          print(f"==> Batch {i}/{MAX_BATCHES}")
          subprocess.run([
              "python", "doc_id_subsystem/core/doc_id_assigner.py",
              "auto-assign", "--limit", str(BATCH_SIZE)
          ])

          subprocess.run(["python", "doc_id_subsystem/core/doc_id_scanner.py"])
          result = subprocess.run(
              ["python", "doc_id_subsystem/core/doc_id_scanner.py", "--stats"],
              capture_output=True, text=True
          )

          for line in result.stdout.splitlines():
              if "Coverage:" in line and "%" in line:
                  coverage_str = line.split("Coverage:")[1].strip().rstrip("%")
                  coverage = float(coverage_str) / 100
                  print(f"Coverage: {coverage * 100:.1f}%")
                  if coverage >= TARGET_COVERAGE:
                      print("Target coverage reached")
                      raise SystemExit(0)

          time.sleep(2)

related_patterns:
  - pattern_id: "EXEC-002"
    name: "Batch Validation"
    relationship: "uses"

  - pattern_id: "PAT-BATCH-FILE-GEN-001"
    name: "Batch File Generation"
    relationship: "inspired_by"

references:
  - doc_id: "DOC-GUIDE-DOC-ID-SYSTEM-OVERVIEW"
    description: "Doc_ID system architecture"

  - doc_id: "DOC-SCRIPT-DOC-ID-ASSIGNER"
    description: "Batch assignment tool"

  - doc_id: "DOC-SCRIPT-DOC-ID-SCANNER"
    description: "Inventory scanner"

notes: |
  This pattern adapts the original batch assignment pattern to the
  eafix-modular layout under doc_id_subsystem/. Use the repo root as the
  working directory for the commands above. Adjust target_coverage or
  batch_size based on repo size and desired coverage.
